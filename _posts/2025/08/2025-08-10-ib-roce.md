---
layout: article
title: IB 原生 RDMA 与 以太网卡 + RoCE 对比
tags: forward
---

## 1. IB 原生 RDMA 方案（InfiniBand 网卡）

**IB = InfiniBand**，是一种高速、低延迟、高带宽的数据中心互连技术，**原生支持 RDMA**，无需额外协议封装。  
典型带宽：40Gbps、100Gbps、200Gbps 及更高。  
主要应用于 **高性能计算（HPC）**、**人工智能训练集群**、**分布式存储** 等对延迟和吞吐极度敏感的场景。

**常见 IB 网卡型号：**
- Mellanox ConnectX-6 (CX6)  
- Mellanox ConnectX-7 (CX7)  
- NVIDIA BlueField 系列 DPU（带 IB 接口）  

**优势：**
- **原生 RDMA**：直接在节点间读写内存，绕过 CPU，延迟可达微秒级。
- 带宽利用率极高，吞吐稳定。
- 在 HPC、AI 集群等场景中有成熟的软硬件生态支持。

**缺点：**
- 成本高，生态依赖特定厂商（如 Mellanox/NVIDIA）。
- 对交换机、拓扑结构、驱动和固件要求严格。

---

## 2. 以太网卡 + RoCE 方案（RDMA over Converged Ethernet）

**RoCE** 是将 RDMA 技术封装到以太网协议中的一种方式，**依赖以太网卡支持 RDMA 功能**（RoCE-capable）。  
其目标是在 Ethernet 环境下获得接近 IB 的 RDMA 性能。

**RoCE 版本：**
- **RoCEv1**：基于二层（L2）以太网，需要同一广播域。
- **RoCEv2**：基于三层（L3）IP 网络，可跨子网部署。

**部署要求：**
- 需要支持 RoCE 的网卡（如 Mellanox ConnectX 系列 Ethernet 版本）。
- 网络需配置 **DCB（数据中心桥接）**，特别是 **PFC（优先级流控）** 来避免丢包，否则 RDMA 性能会大幅下降。

**优势：**
- 成本低于 IB，可利用现有以太网交换机。
- 适合已有大规模 Ethernet 网络的场景，升级成本较小。

**缺点：**
- 延迟、带宽利用率通常低于原生 IB RDMA。
- 网络环境必须无丢包（lossless Ethernet）才能发挥 RoCE 性能。

---

## 3. 对比总结

| 项目         | IB 原生 RDMA（InfiniBand 网卡） | 以太网卡 + RoCE（RDMA over Ethernet） |
| ------------ | ------------------------------ | -------------------------------------- |
| 接口类型     | InfiniBand                     | Ethernet                               |
| 延迟         | 极低（微秒级）                  | 较低（依赖网络配置）                   |
| RDMA 支持    | 原生支持                        | 需要 RoCE-capable 网卡和无丢包网络      |
| 成本         | 高                              | 中等                                   |
| 网络设备依赖 | 需要 IB 交换机                  | 可用以太网交换机（需支持 DCB/PFC）      |
| 典型型号     | CX6、CX7、BlueField IB          | CX6、CX7 Ethernet、Intel E810 等        |
| 应用场景     | HPC、AI 集群、分布式存储        | 数据中心优化型负载、已有 Ethernet 环境   |

---
