---
layout: articles
title: FSDP å¹¶è¡Œç­–ç•¥è§£æ
tags: cpp stl vector 
---




## 1 è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å­˜çš„ç»„æˆ 

åœ¨å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œæ˜¾å­˜å¤§è‡´è¢«ä¸‰ç±»æ•°æ®å ç”¨ï¼š

* **P (Parameters)**ï¼šæ¨¡å‹å‚æ•°
* **G (Gradients)**ï¼šåå‘ä¼ æ’­æ—¶çš„æ¢¯åº¦
* **OS (Optimizer States)**ï¼šä¼˜åŒ–å™¨çš„çŠ¶æ€ï¼ˆå¦‚ Adam çš„åŠ¨é‡å’Œæ–¹å·®ä¿¡æ¯ï¼‰

å…¸å‹çš„ Adam ä¼˜åŒ–å™¨ä¸­ï¼Œè¿™ä¸‰è€…çš„æ¯”ä¾‹å¤§è‡´ä¸º **1 : 1 : 6**

ä¹‹æ‰€ä»¥ Optimizer States å æ¯”é«˜ï¼Œæ˜¯å› ä¸º Adam éœ€è¦ä¸ºæ¯ä¸ªå‚æ•°ç»´æŠ¤ä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰å’ŒäºŒé˜¶çŸ©ï¼ˆæ–¹å·®ï¼‰çš„é¢å¤–ä¿¡æ¯ 

ä¾‹å¦‚ï¼Œåœ¨ PyTorch ä¸­ä¼˜åŒ–å™¨çš„å…¸å‹æ„é€ å¦‚ä¸‹ï¼š

```python
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°çš„è¿‡ç¨‹ï¼š

* `loss.backward()` â†’ ç”Ÿæˆ `parameters.grad`
* `optimizer.step()` â†’ æ›´æ–°ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆOSï¼‰

PyTorch å†…éƒ¨å¤§è‡´é€»è¾‘å¦‚ä¸‹ï¼š

```python
for group in optimizer.param_groups:
    for p in group['params']:
        state = optimizer.state[p]

        # Exponential moving average of gradient values
        m = state['exp_avg']      # åŠ¨é‡å‚æ•°

        # Exponential moving average of squared gradient values
        v = state['exp_avg_sq']   # æ–¹å·®å‚æ•°
```

## 2 æ··åˆç²¾åº¦è®­ç»ƒä¸‹çš„æ˜¾å­˜å ç”¨


åœ¨æ··åˆç²¾åº¦ï¼ˆå‰å‘ FP16ï¼Œåå‘å’Œä¼˜åŒ–å™¨ FP32ï¼‰ä¸‹ï¼Œå¦‚æœæ¨¡å‹æœ‰ `x` ä¸ªå‚æ•°ï¼Œæ˜¾å­˜å ç”¨å¤§è‡´å¦‚ä¸‹ï¼š

| ç±»å‹                            | å ç”¨                           |
| ----------------------------- | ---------------------------- |
| Parametersï¼ˆFP16 + FP32 æ‹·è´ï¼‰    | 2x                           |
| Gradientsï¼ˆFP32ï¼‰               | 2x                           |
| Optimizer Statesï¼ˆAdamï¼Œå…¨ FP32ï¼‰ | 12xï¼ˆ4x å‚æ•°å‰¯æœ¬ + 4x momentum åŠ¨é‡ + 4x variance æ–¹å·®ï¼‰ |




> å‚è€ƒï¼š[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/1910.02054)

ä¸‹å›¾æ˜¯è®ºæ–‡ä¸­é’ˆå¯¹ä¸åŒä¼˜åŒ–å™¨çš„æ˜¾å­˜å ç”¨å¯¹æ¯”ï¼ˆK è¡¨ç¤ºå€æ•°, adamçš„æ—¶å€™K=12ï¼‰ï¼š

<img src="/img/2025/09/DeepSpeed-Image-1.png" alt="Baseline" width="500">

ä¸åŒä¼˜åŒ–å™¨çš„ K å€¼ä¸åŒï¼Œè€Œ DeepSpeed çš„ ZeRO ä¼˜åŒ–å™¨å°±æ˜¯é€šè¿‡å¯¹ P/G/OS è¿›è¡Œåˆ†ç‰‡ï¼Œæ¥é™ä½æ˜¾å­˜å ç”¨çš„ï¼š

* **ZeRO-1**ï¼šåˆ†ç‰‡ OS
* **ZeRO-2**ï¼šåˆ†ç‰‡ OS + G
* **ZeRO-3**ï¼šåˆ†ç‰‡ OS + G + P



æ›´å¤šå…³äº Adam ä¼˜åŒ–å™¨çš„ç»†èŠ‚å¯å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š
ğŸ‘‰ [torch.optim.Adam](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html)

![Adam å‚æ•°è¯´æ˜](/img/2025/09/adam.png)


## 3 DDPï¼ˆData Parallelismï¼‰æ•°æ®å¹¶è¡Œ



åœ¨ DDP æ¨¡å¼ä¸‹ï¼Œæ¯å¼  GPU **ä¿å­˜å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬**ï¼Œå„è‡ªç‹¬ç«‹æ‰§è¡Œå‰å‘ä¸åå‘ï¼Œç„¶åé€šè¿‡ **All-Reduce** æ±‡èšæ¢¯åº¦ã€‚

![DDP AllReduce](/img/2025/09/ddp_allreduce.png)


æµç¨‹å¦‚ä¸‹ï¼š

1. æ¯å¼ å¡ç‹¬ç«‹æ‰§è¡Œå‰å‘è®¡ç®—
2. å„è‡ªè®¡ç®—æ¢¯åº¦
3. ä½¿ç”¨ All-Reduce å¯¹æ‰€æœ‰æ¢¯åº¦æ±‚å’Œï¼Œå¹¶å¹¿æ’­ç»™æ‰€æœ‰ GPU
4. æ¯å¼ å¡ä½¿ç”¨æœ¬åœ°ä¼˜åŒ–å™¨ç‹¬ç«‹æ›´æ–°å‚æ•°

---

## 4 PPï¼ˆPipeline Parallelismï¼‰æµæ°´çº¿å¹¶è¡Œ


PP å°†æ¨¡å‹æŒ‰å±‚æ‹†åˆ†åˆ°ä¸åŒ GPU ä¸Šï¼Œå½¢æˆæµæ°´çº¿ã€‚PyTorch æä¾›äº† `Pipe` APIï¼š

```py

from torch.distributed.pipeline.sync import Pipe

# åˆå§‹åŒ– RPC æ¡†æ¶
os.environ['MASTER_ADDR'] = 'localhost'
os.environ['MASTER_PORT'] = '29500'
torch.distributed.rpc.init_rpc('worker', rank=0, world_size=1)

# æ„å»ºæµæ°´çº¿
fc1 = nn.Linear(16, 8).cuda(0)
fc2 = nn.Linear(8, 4).cuda(1)
model = nn.Sequential(fc1, fc2)
# chunks: number of micro-batches (default: 1)
model = Pipe(model, chunks=8)

input = torch.rand(16, 16).cuda(0)
output_rref = model(input)
```

## 5 FSDPï¼ˆFully Sharded Data Parallelï¼‰å¹¶è¡Œ


è®ºæ–‡ï¼š[Fully Sharded Data Parallel](https://arxiv.org/pdf/2304.11277)

FSDP æ˜¯ä¸€ç§â€œå…¨åˆ†ç‰‡â€æ–¹æ¡ˆï¼Œç›¸æ¯” DDPï¼Œå®ƒèƒ½æå¤§åœ°é™ä½æ˜¾å­˜å³°å€¼å ç”¨ã€‚

### 5.1 æ ¸å¿ƒæ­¥éª¤

1. **å®šä¹‰ FSDP Unit**ï¼šç¡®å®šæŒ‰ layer / module / stage çš„å‚ç›´åˆ‡åˆ†å•å…ƒ
2. **Sharding**ï¼šå¯¹ P / G / OS è¿›è¡Œæ°´å¹³åˆ‡åˆ†
3. **All Gather**ï¼šå‰å‘ä¼ æ’­å‰æ”¶é›†å‚æ•°
4. **Reduce Scatter**ï¼šåå‘ä¼ æ’­åæ¢¯åº¦èšåˆå¹¶åˆ†å‘

![FSDP Overall](/img/2025/09/fsdp_overall.png)

ä¾‹å¦‚ï¼Œä¸€ä¸ª 6 å±‚çš„æ¨¡å‹ï¼Œå¯ä»¥åˆ’åˆ†æˆ 3 ä¸ª unitï¼ˆå¦‚å›¾ layer0+3ã€layer1+2ã€layer4+5ï¼‰ã€‚å…±äº«å‚æ•°çš„ layer éœ€æ”¾åœ¨åŒä¸€ unit ä¸­


### 5.2 FlatParameter ä¸ Sharding

FSDP ä¼šæŠŠæ¯ä¸ª unit çš„æƒé‡å’Œ bias åˆå¹¶ä¸ºä¸€ä¸ª FlatParameterï¼Œç„¶ååœ¨ä¸åŒ GPU ä¸Šè¿›è¡Œåˆ†ç‰‡å­˜å‚¨ï¼š

![alt text](/img/2025/09/unit_sharding.png)



ä¸Šå›¾æè¿°äº†shardingè¿‡ç¨‹ï¼Œ

é¦–å…ˆæŠŠ weight å’Œbias éƒ½å­˜æˆFlatParameterï¼Œå¯èƒ½ä¼šå­˜åœ¨ä¸€å®šçš„padding


FlatParameter å­˜å¥½ä¹‹åï¼Œæ¯å¼ å¡åˆ†åˆ°ä¸€ä»½FlatParameter


- construct units æ„é€  units
    - unit0
    - unit1
    - unit2 
  
- shardingï¼š
    - æŠŠunit å­˜æˆ FlatParameter
    - split FlatParameter åˆ°å¤šä¸ªnode/gpu
    - torch.distributed.fsdp.FullyShardedDataParallel 
        - sharding_strategy
            - FULL_SHARD: os + g + p
            - shard_grad_OP: os + G



### 5.3  ALL gather
![all gather](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/_images/allgather.png)

ä¸Šå›¾æè¿°äº†NCCL çš„all gather åŸè¯­ï¼Œæ¯å¼ å¡å­˜äº†ä¸åŒçš„shardï¼Œä¸»è¦æ˜¯å…ˆconcat ï¼Œå†åšå¹¿æ’­

å‰å‘ä¼ æ’­æ—¶ï¼Œéœ€è¦ä»ä¸åŒ GPU æ”¶é›†åˆ†ç‰‡ â†’ æ‹¼æ¥æˆå®Œæ•´å‚æ•° â†’ è®¡ç®—å®Œåé‡Šæ”¾ã€‚
è¿™ä¸ªè¿‡ç¨‹ä¸è®¡ç®—å¯ **overlap**ï¼Œå³åœ¨è®¡ç®— unit0 çš„å‰å‘æ—¶ï¼Œå¯ä»¥å¹¶è¡Œ gather unit1 çš„å‚æ•°ï¼Œä»è€Œæå‡æ•ˆç‡ã€‚


![FSDP allgather](/img/2025/09/fsdp_allgather.png)

ä¸Šå›¾æ˜¯fsdp ï¼Œåˆ†æˆäº†4ä»½ï¼Œ å‰å‘çš„æ—¶å€™gatherèµ·æ¥ï¼Œå†å¹¿æ’­ï¼Œ ç®—å®Œforward backwardå†é‡Šæ”¾

æ¯å¼ å¡ç»§ç»­ä¿ç•™éƒ¨åˆ†æƒé‡

![alt text](/img/2025/09/overlap_comm_comp.png)

å…¶ä¸­ï¼Œè¿™ç§å‰å‘åå‘çš„è¿‡ç¨‹ï¼Œæ˜¯æœ‰ä¸€å®šçš„overlapå¤„ç†çš„

é¦–å…ˆï¼Œå¯¹unit0 gatherï¼Œ gatheråç®—å‰å‘ï¼Œç®—unit0å‰å‘çš„åŒæ—¶ï¼Œå¯ä»¥å¯¹ unit1 åš gatherï¼Œé€šä¿¡å’Œè®¡ç®—å¯ä»¥åŒæ—¶è¿›è¡Œï¼Œå°±æ˜¯overlap
 

### 5.4 reduce-scatter 

![reduce scatter](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/_images/reducescatter.png)

reduce é»˜è®¤æ“ä½œæ˜¯åŠ å’Œï¼Œ


åå‘ä¼ æ’­åï¼Œå„å¡è®¡ç®—çš„æ¢¯åº¦ä¼šé€šè¿‡ Reduce-Scatter èšåˆå¹¶åˆ†å‘ç»™å„è‡ªè´Ÿè´£çš„åˆ†ç‰‡ã€‚


![ALL reduce ](https://pytorch.org/tutorials/_images/fsdp_sharding.png)


![alt text](/img/2025/09/fsdp_red_scatter.png)

ä¸Šå›¾ï¼Œgatherå®Œä¹‹åï¼Œç®—å®Œä¸åŒçš„æ¢¯åº¦ï¼Œ4å¡çš„æ¢¯åº¦åŠ èµ·æ¥ï¼Œä¸åŒçš„éƒ¨åˆ†å†åˆ†å‘åˆ°ä¸åŒçš„å¡ä¸Š



### 5.5 DDP å’ŒFSDPçš„åŒºåˆ«

* **DDP**ï¼šæ¯å¼  GPU ä¸Šå­˜æ”¾å®Œæ•´æ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨é«˜
* **FSDP**ï¼šæ¯å¼  GPU åªå­˜éƒ¨åˆ†æ¨¡å‹ï¼Œå‰©ä½™æ˜¾å­˜å¯ç”¨äºæ›´å¤§çš„ batch size

![DDP vs FSDP](/img/2025/09/ddp_fsdp.png)

QA: FSDP ä¸‹æ˜¯å¦ä»ä¼šå‡ºç°â€œå³°å€¼æ˜¾å­˜ = æ•´ä¸ªæ¨¡å‹å¤§å°â€ï¼Ÿ

æ˜¯çš„ï¼Œåœ¨ **è®¡ç®—æŸä¸€å±‚æ—¶**ï¼Œè¯¥å±‚çš„å®Œæ•´å‚æ•°ä¼šè¢« gather åˆ°æ‰€æœ‰å‚ä¸ GPU ä¸Šï¼Œæ‰€ä»¥å³°å€¼æ˜¾å­˜ä¼šç¬æ—¶åŒ…å«è¯¥å±‚çš„å®Œæ•´å‚æ•°ã€‚ä½†è¿™æ˜¯**é€å±‚ gather**ï¼Œä¸ä¼šåƒ DDP é‚£æ ·åŒæ—¶å­˜ä¸‹æ•´ä¸ªæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚

## å‚è€ƒï¼š

* [HuggingFace Accelerateï¼šFSDP 1 vs 2](https://huggingface.co/docs/accelerate/concept_guides/fsdp1_vs_fsdp2)
* [PyTorch å®˜æ–¹ FSDP æ•™ç¨‹](https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html)
* [Google Slides - FSDP æŠ€æœ¯è¯¦è§£](https://docs.google.com/presentation/d/1ntPSYg-Wphl8sErwjUl0AztOY1i4SZmQuvmGhkeRElA/edit?slide=id.p#slide=id.p)


---