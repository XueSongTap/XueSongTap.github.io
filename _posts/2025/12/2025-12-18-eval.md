---
layout: article
title: LLM 评估
tags: LLM
---


## 0. 主题与结论预告

- 评测要回答的问题很朴素：在模型已经固定的情况下，它到底“好不好”？
  - 不存在唯一正确的评测；必须先明确你想测量什么，再选择或设计评测。
  - 不要只盯着汇总分数；要回到具体样本与模型预测，理解失败模式。
  - 评测维度不止能力，还包括安全、成本、以及与真实使用场景的贴合度。
  - 必须把“游戏规则”讲清楚：评的是方法，还是模型/系统（含提示词、工具、RAG、脚手架等）。


## 1. 分数与“氛围感”

### 1.1 基准分数

- 图：DeepSeek-R1 基准成绩  
  <img src="/img/2025/12/deepseek-r1-benchmarks.png" width="800" />
  - 引用：DeepSeek-R1 论文（PDF）https://arxiv.org/pdf/2501.12948.pdf
- 图：Llama 4 基准成绩  
  <img src="/img/2025/12/llama4-benchmarks.png" width="800" />
  - 引用：Llama 4（Meta 博客）https://ai.meta.com/blog/llama-4-multimodal-intelligence/
- 图：OLMo 2（32B）  
  <img src="/img/2025/12/https://www.datocms-assets.com/64837/1741887109-instruct-1.png" width="800" />
  - 引用：OLMo 2（32B）（AI2 博客）https://allenai.org/blog/olmo2-32B

- 现象：近年的模型发布通常会拿出一组“看起来熟悉”的基准（例如 MMLU、MATH 等）展示分数。
- 但要警惕“表面可比”的错觉：不同团队使用的基准并不完全一致，评测设置也可能不同（题集版本、提示模板、是否多轮、是否允许工具、采样策略等）。
- 所以看到数字时，不要急着下结论，先问清三件事：
  - 这些基准具体在测什么能力、覆盖什么输入分布？
  - 调用方式是什么（提示词、温度、是否使用 CoT/工具/RAG）？
  - 这个分数与你关心的业务/研究问题之间，距离有多远？

- 图：HELM 能力榜单  
  <img src="/img/2025/12/helm-capabilities-leaderboard.png" width="1000" />
  - 引用：https://crfm.stanford.edu/helm/capabilities/latest/#/leaderboard

- 成本同样是评测的一部分：当效果接近时，价格、延迟、吞吐、上下文长度与稳定性往往决定“到底好不好用”。  
  - 图：Artificial Analysis  
    <img src="/img/2025/12/artificial-analysis.png" width="800" />
  - 引用：https://artificialanalysis.ai/

- 另一种非常现实的“好”的定义：用户是否愿意持续使用，甚至愿意为它付费（这隐含了质量、成本与体验的综合权衡）。  
  - 图：OpenRouter  
    <img src="/img/2025/12/openrouter.png" width="600" />
  - 引用：https://openrouter.ai/rankings

- 图：Chatbot Arena 榜单  
  <img src="/img/2025/12/chatbot-arena-leaderboard.png" width="800" />
  - 引用：https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard

### 1.2 氛围感

- 引用（X/Twitter）：https://x.com/demishassabis/status/1919779362980692364
- 图：Demis / Gemini 2.5（截图）  
  <img src="/img/2025/12/demis-gemini-2.5.png" width="500" />

- 社交媒体上的“氛围感”常常比基准更能影响大众认知：某个模型看起来更会聊天、更像人、更“聪明”，会迅速形成口碑。
- 但主观体验很容易受提示词、样本选择与传播效应影响，不能替代系统化评测；更好的用法是把它当作线索：哪些输入会让人惊艳/失望？失败集中在哪里？
- 图：Karpathy 的“危机感”（截图）  
  <img src="/img/2025/12/karpathy-crisis.png" width="600" />



## 2. 如何思考评测：从抽象目标到具体规则

### 2.1 评测不是“机械跑分”

- 很容易把评测理解成“拿一堆题跑一遍，求个平均分”。但评测真正困难的地方在于：你选择了什么题、怎么问、怎么算分、怎么解释——这些决定了你最终在优化什么
- 换句话说，评测不仅记录模型的发展，还会反过来塑造模型的发展方向。

### 2.2 评测的目的取决于你想回答的问题

- 评测的起点永远是“我要回答什么问题”。典型目标可以分成四类：
  1. 采购/选型：站在用户或公司的角度，在自己的用例上选模型 A 还是 B（例如客服机器人）。
  2. 能力测量：研究者想尽量抽象地衡量模型的通用能力（常被粗略地称作“智能”）。
  3. 收益与风险：理解模型会带来什么收益与危害，用于商业决策或政策讨论。
  4. 研发反馈：模型开发者希望获得可操作的反馈，知道该改哪里、改了是否真的更好。
- 关键：这些都是抽象目标，必须被翻译成具体、可执行、可复现的评测规则。

### 2.3 四步框架：把目标落到规则

- 任何评测都可以拆成四个问题：输入是什么、怎么调用模型、怎么评输出、最后怎么解释指标。

#### (1) 输入：你在测哪些题？

1. 用例覆盖：评测集合覆盖了哪些真实用例？缺了哪些关键用例？
2. 长尾与困难样本：除了“常见且容易”的输入，是否包含足够多的边界情况、困难样本与长尾分布？
3. 形式匹配：输入形式是否贴合模型的真实使用方式（例如是否是多轮对话、是否包含系统提示、是否包含工具返回等）？

#### (2) 调用：你到底怎么用这个模型？

1. 提示词与解码设置：提示模板、温度、top-p、最大长度、是否多次采样，会直接影响结果。
2. 是否引入脚手架：是否使用 CoT、工具调用、RAG、外部记忆、多轮自我修正等？这些会显著改变能力边界。
3. 评测对象要说清：你评的是“裸模型能力”，还是“模型 + 脚手架”的系统能力？前者更利于研究与方法迭代，后者更贴近用户体验与可用性。

#### (3) 输出评估：你把什么算作“对”？

1. 参考答案质量：参考答案是否可靠、是否存在标注错误、是否有多解情况？
2. 指标选择：用准确率、pass@k、胜率、ELO，还是更细的分项指标？不同指标体现不同目标。
3. 成本纳入：除了效果，也要考虑推理成本（价格、延迟、吞吐）以及训练/微调成本。
4. 非对称错误：某些场景里“错一次”的代价远大于“少答一点”（例如医疗幻觉）；评测需要把代价结构写进规则。
5. 开放式生成：当不存在唯一标准答案时，如何评价“好坏”（人评、模型评、规则校验、或把任务改写成可验证形式）？

#### (4) 指标解释：把分数翻译成决策

1. 分数到上线：91% 到底意味着什么？是“可以上线”，还是“某个子场景还不行”？是否需要置信区间与误差条？
2. 泛化与污染：当训练数据可能覆盖了测试数据（或很相似的数据）时，如何判断分数反映的是真泛化还是记忆？
3. 结果归因：你希望通过评测得出“这个模型更好”，还是“这个方法更好”？不同问题需要不同对照设置。

- 小结：评测不是“选个榜单就结束”，而是从目标出发，把规则定义清楚，并持续通过个例检查来校准评测的含义。



## 3. 困惑度：经典但仍有价值

### 3.1 定义与直觉

- 语言模型可以看作对“标记序列”的概率分布 p(x)。
- 困惑度衡量的是：模型是否给数据集 D 分配了足够高的概率。常见写法是  
  - Perplexity = (1 / p(D))^(1/|D|)
- 直觉上可以把它理解为“模型在每一步大概有多少个等可能的选择”，越小越好。
- 困惑度与负对数似然等价，只是表达方式更直观；但可比性依赖于同一数据与分词方式。

### 3.2 训练与评测的关系

- 预训练时优化的目标通常就是降低训练集困惑度（或等价的损失）。
- 因而最直接的评测方式是：在同分布的测试集上测困惑度，观察泛化情况。

### 3.3 传统标准数据集与经典范式

- 传统语言建模常用的标准数据集包括：
  - Penn Treebank（WSJ）
  - WikiText-103（Wikipedia）
  - One Billion Word Benchmark（WMT11，包含 EuroParl、UN、新闻等）
- 经典范式是：在同一数据集的训练划分上训练，在测试划分上评测困惑度。
- 例子：在 One Billion Word Benchmark 上，纯 CNN+LSTM 的困惑度从 51.3 → 30.0  
  - 引用：https://arxiv.org/abs/1602.02410

### 3.4 GPT-2：WebText 训练 + 标准集零样本（OOD）评测

- GPT-2 的做法代表了一个转折：用更大、更“野”的 WebText 训练，然后在传统标准集上做零样本评测。
- 这本质上是分布外（OOD）评测：训练数据与测试数据来源不同，但希望因为训练覆盖足够广而能迁移。
- 图：GPT-2 perplexity  
  <img src="/img/2025/12/gpt2-perplexity.png" width="800" />
- 现象：迁移对小数据集往往更有帮助，但对更大、分布更复杂的数据集（如 1BW）不一定占优。

### 3.5 为什么困惑度仍然有用？

- 背景：从 GPT-2/GPT-3 之后，论文更常用下游任务准确率来展示进步。
- 但困惑度仍然重要，主要有三点：
  - 指标更“平滑”：相较于离散的任务准确率，困惑度更适合用于拟合缩放规律。
  - 更“通用”：困惑度直接对应训练目标，不容易遗漏模型在语言建模上的细微变化。
  - 也可以做“条件困惑度”：在下游任务条件化地计算困惑度，用于缩放分析。  
    - 引用：https://arxiv.org/abs/2412.04403

### 3.6 重要警告：困惑度依赖“可信的概率”

- 如果你在做排行榜，需要意识到困惑度评测要求“信任模型给出的概率”：
  - 做任务准确率时，可以把模型当黑盒：拿到输出，再在外部计算指标。
  - 做困惑度时，需要模型返回每个标记的概率，并相信它们是一个合法的概率分布（历史上遇到 UNK 等问题时会更麻烦）。

### 3.7 “困惑度至上”的观点（及其局限）

- 一种极端观点是：如果真实分布是 t、模型分布是 p，那么当且仅当 p = t 时才能达到最优困惑度；一旦学到真实分布，就能解决所有任务，因此不断降低困惑度最终能通向通用智能。
- 局限在于：即便这个方向“终局正确”，也可能不是最高效的路径，因为你可能在优化分布里“对任务不重要的部分”。

### 3.8 “本质上在测困惑度”的任务：完形填空类

- 有些下游任务表面上不是困惑度，但本质是在测“模型是否能给正确续写更高概率”，因此可以看作困惑度的变体。
- LAMBADA（完形填空）  
  - 引用：https://arxiv.org/abs/1606.06031  
  - 图：  
    <img src="/img/2025/12/lambada.png" width="800" />
- HellaSwag  
  - 引用：https://arxiv.org/pdf/1905.07830  
  - 图：  
    <img src="/img/2025/12/hellaswag.png" width="600" />



## 4. 知识类基准

### 4.1 MMLU

- 引用（MMLU 论文 PDF）：https://arxiv.org/pdf/2009.03300.pdf
- 形式：57 个学科的选择题（例如数学、美国历史、法律、伦理等）。
- 数据来源：由学生从公开网络资料中整理而来。
- 侧重点：更像在测试知识覆盖与考试能力，而不是一般意义上的“语言理解”。
- 图：  
  <img src="/img/2025/12/mmlu.png" width="800" />
- 引用（用于可视化预测）：https://crfm.stanford.edu/helm/mmlu/latest/

### 4.2 MMLU-Pro

- 引用：https://arxiv.org/abs/2406.01574
- 相比 MMLU，MMLU-Pro 的改动方向是“降噪 + 增难”：
  - 移除噪声或过于琐碎的问题。
  - 选项从 4 个扩展到 10 个，降低蒙对概率。
  - 评测时通常允许模型进行更充分的推理（例如 CoT），让模型“有机会把能力发挥出来”。
  - 结果上，多数模型准确率会明显下降，更不容易饱和。
- 图：  
  <img src="/img/2025/12/mmlu-pro.png" width="800" />
- 引用：https://crfm.stanford.edu/helm/capabilities/latest/#/leaderboard/mmlu_pro

### 4.3 GPQA（研究生水平、尽量不靠搜索就能解的问答）

- 引用：https://arxiv.org/abs/2311.12022
- 设计目标是提高题目质量与难度，尽量避免“搜一下就有答案”：
  - 题目由 61 位博士背景的兼职作者撰写。
  - 专家（博士）准确率约 65%。
  - 非专家在可搜索且给足时间的情况下也只有约 34%。
  - GPT-4 在该基准上约 39%（用于说明该基准的挑战性）。
- 图：  
  <img src="/img/2025/12/gpqa.png" width="800" />
- 引用：https://crfm.stanford.edu/helm/capabilities/latest/#/leaderboard/gpqa

### 4.4 Humanity's Last Exam

- 引用：https://arxiv.org/abs/2501.14249
- 规模与形式：2500 题，多模态，多学科，包含选择题与简答题。
- 激励机制：提供奖金池，并给予题目贡献者共同署名等激励，以提高题目质量与多样性。
- 题目筛选：通过前沿模型过滤、多轮人工审阅等流程，尽量避免低质量与泄漏问题。
- 图：示例  
  <img src="/img/2025/12/hle-examples.png" width="800" />
- 图：流程  
  <img src="/img/2025/12/hle-pipeline.png" width="800" />
- 图：结果  
  <img src="/img/2025/12/hle-results.png" width="800" />
- 引用（最新榜单）：https://agi.safe.ai/



## 5. 指令跟随类基准

- 前面的很多基准相对结构化（选择题、固定答案、固定格式）。而“指令跟随”更像真实聊天：用户只给一句话，模型需要理解意图并生成开放式回答。
- 难点：输出往往没有唯一标准答案，因此评测方法本身就成为“规则的一部分”。

### 5.1 Chatbot Arena

- 引用：https://arxiv.org/abs/2403.04132
- 核心机制：随机用户输入提示词，系统随机抽取两个匿名模型回答，用户做二选一比较，再用成对比较计算 ELO。
- 特点：输入来自真实用户、动态更新；可以持续加入新模型；但也会受到用户群体与提示分布变化的影响。
- 图：  
  <img src="/img/2025/12/chatbot-arena-leaderboard.png" width="800" />
- 引用：https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard

### 5.2 IFEval（指令跟随评测）

- 引用：https://arxiv.org/abs/2311.07911
- 图：类别  
  <img src="/img/2025/12/ifeval-categories.png" width="600" />
- 思路：在指令里加入一些可自动验证的“硬约束”（例如输出格式、必须包含/不包含某些元素）。
- 优点：约束是否满足通常可以程序化检查，成本低、可复现。
- 局限：只能验证形式与局部约束，很难覆盖语义是否真正满足用户需求；约束也可能相对人工与不自然。
- 引用：https://crfm.stanford.edu/helm/capabilities/latest/#/leaderboard/ifeval

### 5.3 AlpacaEval

- 引用：https://tatsu-lab.github.io/alpaca_eval/
- 数据：805 条来自不同来源的指令。
- 指标：用模型评审计算“对 GPT-4 preview 的胜率”，评审者也是 GPT-4 preview，因此可能带来偏差。
- 图：  
  <img src="/img/2025/12/alpacaeval-leaderboard.png" width="600" />

### 5.4 WildBench

- 引用：https://arxiv.org/pdf/2406.04770
- 数据：从大量真实人机对话中抽取样本（例如从 100 万条里抽 1024 条），更贴近真实分布。
- 评审：使用模型评审并配合清单式打分，让评审过程更结构化，并进行多模型交叉评审。
- 经验现象：与 Chatbot Arena 相关性很高，可作为基准之间的一种“对齐检查”。
- 图：  
  <img src="/img/2025/12/wildbench.png" width="800" />
- 引用：https://crfm.stanford.edu/helm/capabilities/latest/#/leaderboard/wildbench



## 6. 智能体类基准（需要工具与长程迭代）

- 很多真实任务不仅是“回答一句话”，还需要调用工具（写代码、跑测试、查资料），并在多步迭代中逐步接近目标。
- 这里的“智能体”可以理解为：语言模型 + 外围脚手架（用于决定何时调用模型、何时调用工具、如何记忆与反思）。

### 6.1 SWEBench

- 引用：https://arxiv.org/abs/2310.06770
- 任务：来自 12 个 Python 仓库的 2294 个真实问题修复/功能实现任务。
- 输入：给定代码库与 issue 描述，目标是提交一个可合入的修复（类似提交 PR）。
- 指标：用单元测试作为最终判定（修复是否通过、是否引入回归）。
- 图：  
  <img src="/img/2025/12/swebench.png" width="800" />

### 6.2 CyBench

- 引用：https://arxiv.org/abs/2408.08926
- 场景：网络安全类 CTF 任务（40 个）。
- 难度度量：以首次解出时间等信号来衡量任务难度与系统表现。
- 图：  
  <img src="/img/2025/12/cybench.png" width="800" />
- 图：agent  
  <img src="/img/2025/12/cybench-agent.png" width="800" />
- 图：结果  
  <img src="/img/2025/12/cybench-results.png" width="800" />

### 6.3 MLEBench

- 引用：https://arxiv.org/abs/2410.07095
- 场景：Kaggle 竞赛型任务（75 个），通常需要下载/处理数据、训练模型、做特征工程与反复提交。
- 图：  
  <img src="/img/2025/12/mlebench.png" width="800" />
- 图：结果  
  <img src="/img/2025/12/mlebench-results.png" width="800" />



## 7. 纯推理基准

- 前面很多基准都混杂了语言能力与世界知识：模型到底是“会推理”，还是“背得多”？能不能把推理从知识中尽量剥离出来？
- 一种观点认为：推理可能更接近“纯粹的智能成分”，至少不只是记忆事实。

- ARC-AGI  
  - 引用：https://arcprize.org/arc-agi
  - 背景：2019 年由 Francois Chollet 提出

### 7.1 ARC-AGI-1

- 图：任务网格示例  
  <img src="/img/2025/12/https://arcprize.org/media/images/arc-task-grids.jpg" width="800" />
- 图：榜单  
  <img src="/img/2025/12/https://arcprize.org/media/images/oseriesleaderboard.png" width="800" />

### 7.2 ARC-AGI-2（更难）

- 目标：进一步提高难度，让“刷题式”策略更难奏效。
- 图：  
  <img src="/img/2025/12/https://arcprize.org/media/images/blog/arc-agi-2-unsolved-1.png" width="800" />



## 8. 安全评测

- 图（crash test，类比安全测试）  
  <img src="/img/2025/12/https://www.team-bhp.com/forum/attachments/road-safety/2173645d1625144681-will-crash-test-rating-change-if-higher-variant-chosen-images-30.jpeg" width="500" />
- 引用（HELM safety 榜单）：https://crfm.stanford.edu/helm/safety/latest/#/leaderboard

### 8.1 HarmBench

- 引用：https://arxiv.org/abs/2402.04249
- 基于 510 类违反法律或社会规范的有害行为，评测模型在有害请求上的表现（例如是否会给出危险指导）。
- 引用：https://crfm.stanford.edu/helm/safety/latest/#/leaderboard/harm_bench
- 引用（安全失败样例）：https://crfm.stanford.edu/helm/safety/latest/#/runs/harm_bench:model=anthropic_claude-3-7-sonnet-20250219?instancesPage=4

### 8.2 AIR-Bench

- 引用：https://arxiv.org/abs/2407.17436
- 基于监管框架与公司政策构建风险分类体系（314 个风险类别、5694 条提示）。
- 图：  
  <img src="/img/2025/12/https://crfm.stanford.edu/helm/assets/air-overview-d2e6c49f.png" width="800" />
- 引用：https://crfm.stanford.edu/helm/air-bench/latest/#/leaderboard

### 8.3 越狱（Jailbreaking）

- 现实对抗：模型通常被训练去拒绝有害指令，但用户/攻击者会通过越狱提示绕过安全策略。
- 例如 GCG（Greedy Coordinate Gradient）可以自动优化提示词，寻找更容易绕过的攻击。  
  - 引用：https://arxiv.org/pdf/2307.15043
- 越狱提示可能在模型之间迁移：在开源权重模型上找到的策略，有时能迁移到闭源模型上。
- 图：GCG examples  
  <img src="/img/2025/12/gcg-examples.png" width="800" />

### 8.4 部署前测试

- 一种做法是：在模型发布前，由独立机构进行评测并出具报告（目前多为自愿合作）。
- 例如美国与英国的 AI 安全机构合作，在发布前获得模型访问权限，进行系统化测试。
- 引用（报告）：https://www.nist.gov/system/files/documents/2024/12/18/US_UK_AI%20Safety%20Institute_%20December_Publication-OpenAIo1.pdf

### 8.5 安全到底是什么？

- 安全高度依赖语境：政治、法律、社会规范在不同国家/文化下差异很大。
- 直觉上容易把安全理解成“拒绝有害内容”，甚至认为安全与能力是对立的；但实际更复杂。
- 例如在医疗场景中，减少幻觉不仅提升安全性，也提升系统的实用能力（因为用户更敢用、也更能用）。

### 8.6 两个维度：能力与倾向

- 用两个维度理解安全风险更清晰：
  - 能力（capabilities）：模型是否做得到某件危险的事。
  - 倾向（propensity）：模型在被请求时是否愿意去做（是否会配合输出）。
- 同一系统可能“做得到但拒绝做”，因此对 API 模型来说，倾向往往更关键；而对开源权重模型来说，能力更关键（因为安全倾向更容易被微调或绕开）。

### 8.7 双重用途（Dual-use）

- 双重用途：强大的网络安全智能体（例如在 CyBench 上表现好）既能用于渗透测试，也可能被用于入侵攻击。
- 这会带来边界问题：某些“安全评测”到底是在评风险，还是在评能力？同一个基准可能同时具有两种含义。



## 9. 现实性：评测与真实使用的距离

- 现实背景：语言模型在实践中被大规模使用，真实流量与开发投入都在快速增长。
- 图：OpenAI tokens（引用推文）  
  <img src="/img/2025/12/openai-100b-tokens.png" width="600" />
  - 引用（tweet）：https://x.com/sama/status/1756089361609981993
- 图：Cursor lines（引用推文）  
  <img src="/img/2025/12/cursor-1b-lines.png" width="600" />
  - 引用（tweet）：https://x.com/amanrsanger/status/1916968123535880684

- 但现实性很难：很多基准（例如 MMLU）离真实使用场景很远；而直接用真实流量又会混入大量噪声与隐私问题，也未必适合作为“想要的评测”。

### 9.1 两类提示：考你与求助

- 可以把用户提示粗略分成两类：
  1. 考你（Quizzing）：用户知道答案，目的是测试系统（类似标准化考试）。
  2. 求助（Asking）：用户不知道答案，目的是用系统解决问题、获得价值。
- 从真实价值的角度，“求助型”更贴近实际使用，但也更难评测。

### 9.2 Clio（Anthropic）

- 引用：https://arxiv.org/abs/2412.13678
- 思路：用语言模型分析真实用户数据，提炼“人们到底在问什么”的总体模式（在保护隐私的前提下共享更抽象的统计与结构）。
- 图：  
  <img src="/img/2025/12/clio-table4.png" width="700" />

### 9.3 MedHELM

- 引用：https://arxiv.org/abs/2412.13678
- 背景：许多传统医疗基准更像考试题，未必贴近临床工作流。
- 做法：由 29 位临床医生贡献，构建 121 个临床任务，混合私有与公开数据源，更强调真实工作场景的代表性。
- 图：  
  <img src="/img/2025/12/https://crfm.stanford.edu/helm/assets/medhelm-overview-3ddfcd65.png" width="700" />
- 引用：https://crfm.stanford.edu/helm/medhelm/latest/#/leaderboard

- 小结：现实性与隐私经常存在张力；越贴近真实数据，越需要更强的隐私保护与合规策略。



## 10. 有效性：我们如何知道评测“靠谱”？

- “有效性”关心的是：这个评测分数，是否真的代表了你想测的东西？是否能支持你要做的判断？

### 10.1 训练-测试重叠（污染）

- 机器学习常识：不要在测试集上训练。
- 在基础模型之前，很多任务有明确的训练/测试划分（例如 ImageNet、SQuAD），因此更容易保证评测有效。
- 但今天的大模型往往训练于互联网语料，且数据细节不透明，导致训练-测试重叠更难避免，也更难证明不存在。

#### 路线 1：从模型行为推断重叠

- 通过模型在样本上的表现来反推是否存在污染，并利用数据点的“可交换性”等性质进行统计检验。  
  - 引用：https://arxiv.org/pdf/2310.17623
- 图：contamination / exchangeability  
  <img src="/img/2025/12/contamination-exchangeability.png" width="600" />

#### 路线 2：推动更强的报告规范

- 鼓励更规范的报告（例如给出置信区间、评测设置细节）。
- 也推动模型提供方报告训练-测试重叠情况（至少报告他们能检查到的部分）。  
  - 引用：https://arxiv.org/abs/2410.08385

### 10.2 数据集质量

- 评测本身也会“坏”：题目表述不清、答案错误、环境不可复现，都会把噪声引入分数。
- 因此会出现对基准的“修订版/验证版”：
  - 例如修复 SWE-Bench 得到 SWE-Bench Verified。  
    - 引用（blog）：https://openai.com/index/introducing-swe-bench-verified/
  - 或者构建更高质量的“白金版”基准集合。  
    - 引用：https://arxiv.org/abs/2502.03461
- 图：  
  <img src="/img/2025/12/https://pbs.twimg.com/media/GjICXQlWkAAYnDS?format=jpg&name=4096x4096" width="700" />
- 图：  
  <img src="/img/2025/12/https://pbs.twimg.com/media/GjICcGQXYAAM4o1?format=jpg&name=4096x4096" width="800" />



## 11. 我们到底在评什么：方法 vs 模型/系统

- 评测本质上是在定义一场“比赛”的规则：参赛者是什么？允许使用哪些资源？如何判定胜负？
- 一个重要变化是：
  - 过去（基础模型之前）：更多在评“方法”（固定数据与标准化训练-测试划分，比较算法改进）。
  - 现在：更多在评“模型/系统”（提示词、工具、RAG、脚手架等都可能变化，甚至“任何设置都行”）。

### 11.1 更像评“方法”的例外

#### nanogpt speedrun

- 固定数据与目标验证损失，比较达到目标所需的计算时间/训练效率。
- 图：  
  <img src="/img/2025/12/karpathy-nanogpt-speedrun.png" width="600" />
- 引用（X/Twitter）：https://x.com/karpathy/status/1846790537262571739

#### DataComp-LM

- 给定原始数据集，在标准训练流程下追求最佳效果，更强调数据处理与训练策略的系统性比较。
- 引用：https://arxiv.org/abs/2406.11794

### 11.2 方法评测 vs 系统评测：各自价值

- 方法评测的价值：鼓励研究者在算法与训练方法上创新，因为对照条件更清晰。
- 系统评测的价值：更贴近下游用户关心的“拿来能不能用”，尤其当系统真实形态本来就包含提示词与工具。
- 无论评哪一种，最关键的是把规则写清楚：比较对象是什么、允许什么、不允许什么、以及如何复现。



## 12. 总结

- 没有“唯一正确”的评测：先定义你要回答的问题，再选择/设计输入、调用方式与指标。
- 不要迷信排行榜：分数的可比性依赖于题集与评测设置；同时要结合成本、稳定性与场景匹配度。
- 以个例驱动理解：回到具体样本与预测，定位失败模式，才能知道分数变化意味着什么。
- 评测要覆盖多维度：能力只是其中一维；安全、成本、现实性往往决定能否落地。
- 把规则讲清楚：你评的是方法还是模型/系统；是否允许工具、RAG、脚手架；如何复现与解释结果。
