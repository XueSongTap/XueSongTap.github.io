---
layout: article
title: NCCL 通信原语 (Primitives)
tags: LLM
---



#### 1 Broadcast
从 **root rank** 广播数据到所有设备。



![nccl broadcast](/img/2025/10/nccl_broadcast.png)

接口：
```c
ncclResult_t ncclBroadcast(const void* sendbuff, void* recvbuff,
                            size_t count, ncclDataType_t datatype,
                            int root, ncclComm_t comm, cudaStream_t stream)
```
#### 2 Reduce

执行规约计算（如 **max, min, sum**），并将结果写入指定的 rank。


![nncl reduce](/img/2025/10/nccl_reduce.png)

```c
ncclResult_t ncclReduce(const void* sendbuff, void* recvbuff,
                        size_t count, ncclDataType_t datatype, ncclRedOp_t op,
                        int root, ncclComm_t comm, cudaStream_t stream)
```
#### 3 ReduceScatter

计算规约算子，然后 把结果分到不同的 rank

![alt text](/img/2025/10/nccl_reducescatter.png)

```c
ncclResult_t ncclReduceScatter(const void* sendbuff,
                                void* recvbuff , size_t recvcount, ncclDataType_t datatype,
                                ncclRedOp_t op, ncclComm_t comm, cudaStream_t stream)
```

#### 4 AllGather

从 $k$ 个 rank 收集各自的 $N$ 个值，形成大小为 $k \times N$ 的输出，并广播给所有 rank。


![alt text](/img/2025/10/nccl_allgather.png)

```c
ncclResult_t ncclAllGather(const void* sendbuff,
                            void* recvbuff, size_t sendcount, ncclDataType_t datatype,
                            ncclComm_t comm, cudaStream_t stream)
```

#### 5 AllReduce

等价于 **Reduce + Broadcast**（或 ReduceScatter + AllGather）。

就是reuduce 后 再广播 下

![alt text](/img/2025/10/nccl_allreduce.png)
```c
ncclResult_t ncclAllReduce(const void* sendbuff,
                            void* recvbuff , size_t count, ncclDataType_t datatype,
                            ncclRedOp_t op, ncclComm_t comm, cudaStream_t stream)
```

